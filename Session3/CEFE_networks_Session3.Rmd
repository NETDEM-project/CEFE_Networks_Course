---
title: 'CEFE Social Networks Course: Session 3'
author: "Matthew Silk"
date: "2023-04-10"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Set up R environment

First we need to load the R packages we will use. You may need to install these if you haven't used them before using the `install.packages()` command in R. For genNetDem you will need to follow the instructions [here](https://github.com/NETDEM-project/genNetDem) to install it.

```{r packages, warning=FALSE,message=FALSE}

set.seed(1)

library(igraph)
library(asnipe)
library(viridis)
library(assortnet)
library(genNetDem)
library(brms)
library(Matrix)
library(sna)

```

***

## SESSION 3: PART 2

Our first step in this session is to work through code for how generate permutation-based reference models for networks. For a long time permutation methods were seen as a universal tool in animal social network analysis, with similar methods applied broadly across diverse research questions. We now know that using permutation-based tests in this way is not the best idea. For example, choosing the wrong type of permutation test can [lead to issues with false positives]( https://doi.org/10.1111/2041-210X.13508) or mean that [we are not controlling for non-independence any better than we would be by using more conventional statistical approaches](https://link.springer.com/article/10.1007/s00265-022-03254-x). Another drawback of permutation methods is that they are [not well tailored to combining with the interpretation of effect sizes](https://doi.org/10.1111/2041-210X.13429).

However, [reference model approaches](https://doi.org/10.1111/brv.12775) can still represent a useful tool for social network analysists. This is especially true when comparing the networks you observe in your study system to reference models generated by randomising (or permuting) data with respect to key features of interest or using agent-based models. By exploring how to conduct network permutations it can help us visualise various key aspects of network analysis that will come up again in more complex approaches. It also provides some initial insights into how we might develop generative (or agent-based) models for networks. Therefore, it represents a sensible starting point before we move on to more state-of-the art methods.

I would also recommend that a good starting point is to always (when possible) write out your own models to conduct permutations (incorporating specific algorithms when useful) as this gives you greater control over the features you maintain.

We will start our analysis with the most basic type of permutation test, in which we swap features of nodes or edges. This will use the `full_adj` network from the previous sessions. We will show node swap permutations in the context of testing whether there is assortativity in social network connections by sex - that is are males more likely to interact with other males, and females with other females. This is something that we might commonly want to test with social network analyses and is in fact a simple version of a dyadic regression (see below) For the calculation of assortativity we will use a new package [assortnet](https://cran.r-project.org/web/packages/assortnet/assortnet.pdf).

```{r node_swaps}

#Read in the network object
full_adj<-readRDS("full_adj.RDS")
#Read in associated trait data
ind_data<-readRDS("ind_data.RDS")
names(ind_data)[1]<-"id"

#Create a weighted version of full_adj - we are adding edge weights from a beta distribution
full_adj_weight<-rbeta(sum(upper.tri(full_adj)),2,7)
full_adj[upper.tri(full_adj)]<-full_adj_weight*full_adj[upper.tri(full_adj)]
full_adj[lower.tri(full_adj)]<-t(full_adj)[lower.tri(full_adj)]

#Create igraph network object
network<-igraph::graph_from_adjacency_matrix(full_adj,mode="undirected",weighted=TRUE)
V(network)$size<-5+ind_data$offspring
V(network)$sex<-ind_data$sex
V(network)$group<-ind_data$group

#Calculate assortativity in the observed network
obs_assort<-assortnet::assortment.discrete(graph=full_adj,types=ind_data$sex,weighted=TRUE,SE=FALSE)$r

#For node feature swaps in this context we can simply permute or randomise the sex of each individual
#Without constraints this can simply use the sample function

#We calculate our reference distribution
perm_assort<-numeric()
for(i in 1:9999){
  perm_assort[i]<-assortnet::assortment.discrete(graph=full_adj,types=sample(ind_data$sex,nrow(ind_data),replace=FALSE),weighted=TRUE,SE=FALSE)$r
}

#We can plot the outcome of our permutation test like this:
hist(perm_assort,main="",breaks=50,las=1,xlab="Reference distribution of assortativity values")
#And add our observed assortativity to this plot
lines(x=rep(obs_assort,2),y=c(-1000,10000),col="red",lwd=3)

#We can already see that it is unlikely that individuals in our network our more assorted by sex than you would expect by  chance.
#But we can confirm this by calculating a p value

#We first add the observed value to the reference distribution for p value calculation
perm_assort2<-c(perm_assort,obs_assort)

#We can then calculate what proportion of the reference distribution is greater than our observed assortativity (a proportion smaller than 0.025 or greater than 0.975 would indicate statistical significance for alpha=0.05)
sum(obs_assort<perm_assort2)/length(perm_assort2)

#As expected the result is not statistically significant

```

This is a basic node feature permutation. We could make these permutations more sophisticated by constraining the potential swaps we could make. There are a couple of examples provided below - constraining swaps according to group membership ad constraining swaps to be between individuals with the same degree. Adding these constraints adds more nuance to our reference model and null hypothesis so that we can ask: a) are networks assorted by sex once we account for variation in sex ratio among groups?; and b) are networks assorted by sex once we account for between-individual variation in degree?. You can see that each question/hypothesis can be very similar if we are not careful with the wording we choose - it is important to pick your reference model and permutation procedure very carefully.

```{r node_swaps2}

##Permutations constrained to be within the same group

#We change our sampling algorithm
perm_assort<-numeric()
for(i in 1:9999){
  groups<-unique(ind_data$group)
  new_sex<-ind_data$sex
  for(j in 1:length(groups)){
    g_j<-which(ind_data$group==groups[j])
    g_j2<-sample(g_j,length(g_j),replace=FALSE)
    new_sex[g_j]<-new_sex[g_j2]
  }
  perm_assort[i]<-assortnet::assortment.discrete(graph=full_adj,types=new_sex,weighted=TRUE,SE=FALSE)$r
}

#We can plot the outcome of our permutation test like this:
hist(perm_assort,main="",breaks=50,las=1,xlab="Reference distribution of assortativity values")
#And add our observed assortativity to this plot
lines(x=rep(obs_assort,2),y=c(-1000,10000),col="red",lwd=3)

#We first add the observed value to the reference distribution for p value calculation
perm_assort2<-c(perm_assort,obs_assort)

#We can then calculate what proportion of the reference distribution is greater than our observed assortativity (a proportion smaller than 0.025 or greater than 0.975 would indicate statistical significance for alpha=0.05)
sum(obs_assort<perm_assort2)/length(perm_assort2)

##Permutations constrained by the degree of individuals

#We use a similar algorithm to the previous example. We just have to calculate degree first this time
perm_assort<-numeric()
for(i in 1:9999){
  deg<-igraph::degree(network)
  u_deg<-unique(deg)
  new_sex<-ind_data$sex
  for(j in 1:length(u_deg)){
    g_j<-which(deg==u_deg[j])
    g_j2<-sample(g_j,length(g_j),replace=FALSE)
    new_sex[g_j]<-new_sex[g_j2]
  }
  perm_assort[i]<-assortnet::assortment.discrete(graph=full_adj,types=new_sex,weighted=TRUE,SE=FALSE)$r
}

#We can plot the outcome of our permutation test like this:
hist(perm_assort,main="",breaks=50,las=1,xlab="Reference distribution of assortativity values")
#And add our observed assortativity to this plot
lines(x=rep(obs_assort,2),y=c(-1000,10000),col="red",lwd=3)

#We first add the observed value to the reference distribution for p value calculation
perm_assort2<-c(perm_assort,obs_assort)

#We can then calculate what proportion of the reference distribution is greater than our observed assortativity (a proportion smaller than 0.025 or greater than 0.975 would indicate statistical significance for alpha=0.05)
sum(obs_assort<perm_assort2)/length(perm_assort2)


```

Unsurprisingly in this case, adding these constraints does not provide any additional information - there is no assortment by sex to start with and we didn't simulate the network to have very different sex ratios between groups or highly variable degree distributions.

As well as constraining swaps according to biological features such as these, it would also be possible to constrain swaps according to features of the sampling design or constraints such as the number of times an individual was observed.

Note that the degree example is a nice one to illustrate a potential issue with permutations in small networks, especially if they have many constraints. If we look at the degree distribution of the network we can see that there is only one individual with a degree of 4 meaning with the current constraints this individual can never be swapped with another! Similarly there are only a few individuals with a degree of 5 or degree of 11. While the degree distribution in our network is relatively homogeneous so this is not a major constraint, in more heterogeneous degree distributions this could cause major issues with the number of potential permuted datasets that it is possible to achieve.

```{r node_swaps3}

table(deg)

```

Edge feature swaps work in much the same way as node feature swaps but using the edge list and edge traits instead of the dataframe of individuals and node metrics.

***
***

We can now introduce edge rewiring algorithms, which we can use to test an overlapping set of hypotheses in network analysis. Once again it is important to use edge rewiring very carefully, and be specific when matching the hypothesis and permutation procedure.

In the examples here, we are going to focus on analyses related to contagions and transmission in the network, using the global efficiency measure from Session 2 as a proxy for how quickly a social contagion (such as a new behaviour) might spread through the network. We are going to start with the "*easy*" undirected version. and then mover on to the more difficult version where we write our own algorithm to do the edge rewiring. While the latter is more difficult, it is a good skill to have as it provides much more flexibility in our analyses.

To make things easier we will work with the binary (not weighted) version of `full_adj`.

For basic rewiring approaches `igraph` has us covered. We are going to test two hypotheses: a) our `full_adj` network is less efficient than a random network; and b) less efficient than a network with the same degree distribution but without other aspects of the observed network structure (e.g. the presence of communities).

```{r igraph_ew}

#Read in the network object
full_adj2<-readRDS("full_adj.RDS")

#Create igraph network object
network2<-igraph::graph_from_adjacency_matrix(full_adj2,mode="undirected")
V(network)$size<-5+ind_data$offspring
V(network)$sex<-ind_data$sex
V(network)$group<-ind_data$group

#Calculate the observed efficiency
obs_eff<-igraph::global_efficiency(network2)

#Create reference distribution for the first comparison using the rewire() function in igraph with its each_edge argument
perm_effs1<-numeric()
for(i in 1:9999){
  ref_net<-igraph::rewire(network2,with=each_edge(prob=1,loops=FALSE,multiple=FALSE))
  perm_effs1[i]<-igraph::global_efficiency(ref_net)
}

#Create reference distribution for the second comparison using the rewire() function in igraph with its keeping_degseq argument
#The niter is the number of iterations of swaps to generate the new network
perm_effs2<-numeric()
for(i in 1:9999){
  ref_net<-igraph::rewire(network2,with=keeping_degseq(loops=FALSE,niter=2000))
  perm_effs2[i]<-igraph::global_efficiency(ref_net)
}

#Let's plot the outcome of our permutations in histogram form as before
hist(perm_effs1,main="",breaks=seq(0.38,0.48,0.001),col=adjustcolor("firebrick",0.5),border=NA,ylim=c(0,3000),las=1,xlab="Global Efficiency")
hist(perm_effs2,main="",breaks=seq(0.38,0.48,0.001),col=adjustcolor("dodgerblue",0.5),border=NA,add=TRUE)
lines(x=rep(obs_eff,2),y=c(-1000,100000),lwd=3)

#We could calculate p values as before, but the results is fairly obvious here!

```

This example, illustrates a couple of nice points about using reference model approaches. By using multiple reference models simultaneously we can (potentially) learn more about the patterns in our observed network. We can also learn from the comparison of reference models with each other - in this case the global efficiency of our reference networks is the same for both reference models. This strongly indicates that the degree distribution of our observed network is not important in influencing its global efficiency.

However, we might want to move beyond what the functionality in igraph can offer us in terms of edge rewiring, and add our own constraints. Once again you could do this for diverse features, both biological (group, sex etc.) or related to sampling. The same considerations as before apply, it is important to think about what features of the network your permutations will change and if there are enough possible swaps once you impose all of your constraints.

In the example below we are going to let the degree distribution vary but only allow swaps within social groups. We will use this to test the hypothesis that membership of social groups is what is causing the efficiency of our network to be low.

Now we are coding our permutations manually we are going to use a Markov chain approach, which is illustrated step-by-step below. Here we will just generate one chain, but note that for formal analyses it is preferable to [use multiple chains](https://osf.io/xkvcu) in most cases. Some of you will know Markov chains from Bayesian modelling approaches, for others this will be new but we will take it step by step and I can help with anything that is unclear. We will include a burn-in and thinning interval just as you would consider for Bayesian analyses.

```{r manual_ew}

##Conduct edge rewiring while constraining swaps to be within social groups

#We will have a burn-in of 500 swaps and then sample the network every 10 swaps until we have the full reference distribution.

#Set the parameters for the chain
burnin<-500
thin<-10
chain<-100000

#Set up the vector to store the reference distribution
perm_effs3<-numeric()

#Create our reference network (in adjacency matrix format)
ref_mat<-full_adj2

#We now conduct our swaps in a slower but easier to explain way so I can explain the logic as we go
for(i in 1:burnin){
  #Choose a first individual
  ind1<-sample(1:nrow(ref_mat),1)
  #Record its group
  t_group<-ind_data$group[ind1]
  #Create a vector of all other individuals in the group
  poss<-which(ind_data$group==t_group)
  poss<-poss[poss%in%ind1==FALSE]
  #Choose a second individual. The first and second individuals (ind1 and ind2) will be the first edge of the two to swap
  ind2<-sample(poss,1)
  #Remove ind2 from the vector
  poss<-poss[poss%in%ind2==FALSE]
  #Choose a third individual
  ind3<-sample(poss,1)
  #Remove that individual from the vector
  poss<-poss[poss%in%ind3==FALSE]
  #Choose a fourth individual.ind3 and ind4 will be the second edge for the swap
  ind4<-sample(poss,1)
  #Conduct the edge swap so that now the edge ind1--ind2 has the value of ind3--ind4 and vice versa
  t_ref_mat<-ref_mat
  ref_mat[ind1,ind2]<-ref_mat[ind2,ind1]<-t_ref_mat[ind3,ind4]
  ref_mat[ind3,ind4]<-ref_mat[ind4,ind3]<-t_ref_mat[ind1,ind2]
}

#We now continue our sampling but every 10 iterations we calculate the efficiency of the network. The counter helps add to the reference distribution.
counter<-1
for(i in 1:chain){
  #Choose a first individual
  ind1<-sample(1:nrow(ref_mat),1)
  #Record its group
  t_group<-ind_data$group[ind1]
  #Create a vector of all other individuals in the group
  poss<-which(ind_data$group==t_group)
  poss<-poss[poss%in%ind1==FALSE]
  #Choose a second individual. The first and second individuals (ind1 and ind2) will be the first edge of the two to swap
  ind2<-sample(poss,1)
  #Remove ind2 from the vector
  poss<-poss[poss%in%ind2==FALSE]
  #Choose a third individual
  ind3<-sample(poss,1)
  #Remove that individual from the vector
  poss<-poss[poss%in%ind3==FALSE]
  #Choose a fourth individual.ind3 and ind4 will be the second edge for the swap
  ind4<-sample(poss,1)
  #Conduct the edge swap so that now the edge ind1--ind2 has the value of ind3--ind4 and vice versa
  t_ref_mat<-ref_mat
  ref_mat[ind1,ind2]<-ref_mat[ind2,ind1]<-t_ref_mat[ind3,ind4]
  ref_mat[ind3,ind4]<-ref_mat[ind4,ind3]<-t_ref_mat[ind1,ind2]
  
  #Here we calculate global efficiency for the selected iteractions of the Markov Chain
  if(i%%thin==0){
    ref_net<-igraph::graph_from_adjacency_matrix(ref_mat,mode="undirected")
    perm_effs3[counter]<-igraph::global_efficiency(ref_net)
    counter<-counter+1
  }
  
}

#Let's plot the outcome of our permutations in histogram form as before
hist(perm_effs1,main="",breaks=seq(0.38,0.48,0.001),col=adjustcolor("firebrick",0.5),border=NA,ylim=c(0,3000),las=1,xlab="Global Efficiency")
hist(perm_effs2,main="",breaks=seq(0.38,0.48,0.001),col=adjustcolor("dodgerblue",0.5),border=NA,add=TRUE)
hist(perm_effs3,main="",breaks=seq(0.38,0.48,0.001),col=adjustcolor("goldenrod3",0.5),border=NA,add=TRUE)
lines(x=rep(obs_eff,2),y=c(-1000,100000),lwd=3)

```

Our observed efficiency measure lies right in the centre of the third reference distribution. Therefore we can included that interactions being clustered within social groups is important in explaining the efficiency of our network. Again, we could test this formally as we did previously, in order to do this you would (ideally) need to run multiple chains. If you are feeling confident then feel free to give this a go!

***
***

In some types of animal social network dataset, we may want to conduct permutations in the dataset used to construct the network instead. This can be useful in [GPS data](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12553) or [data on group memberships](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12772), for example. However, with these datastream permutations it is necessary to be particularly careful with how they are used. They can be useful for comparing to reference models about how changes to the raw data might impact the structure of the network - for example, with group-based networks you could ask *Is my social network different to if individuals occurred in groups at random?*. However, because the swaps are conducted not on the social network it is harder to know what features of the network you are maintaining and so it can [lead to problems when applied to nodal or dyadic regressions](https://doi.org/10.1111/2041-210X.13508) - hence why below I suggest more conventional statistical tools to address these questions. 

We first need to generate some grouping events based on our network to demonstrate these datastream permutations. We can use my [genNetDem R package](https://github.com/NETDEM-project/genNetDem).

```{r generate_gbi}

#Note we need to slightly change the format of our ind_data object here to suit the package
t_ind_data<-ind_data
names(t_ind_data)[1]<-"indivs"

#We can then use the interaction_generation_simul function to generate groups based on our network
groupings<-genNetDem::interaction_generation_simul(indiv_data=t_ind_data,pop_mat=full_adj,mean_group_size=3,n_ts=20)

#We can create a network using this group-by-individual matrix and plot it to see what it looks like
group_mat<-asnipe::get_network(groupings[[1]])
group_net<-igraph::graph_from_adjacency_matrix(group_mat,mode="undirected",weighted=TRUE)

plot(group_net,vertex.size=15,vertex.color="gray10",vertex.label.color="white",vertex.label.cex=0.75)

#We also have information available on the times that groupiing events were observed at
day<-groupings[[2]]
day

```

So we now have a social network based on observations of a series of grouping events that resembles the weighted social network we started with. We are going to use this to demonstrate datastream permutations - showing how they can be used to ask the question *Is my network different to if individuals formed groups randomly?*. For datastream permutations of group-by-individual matrices (or incidence matrices) the process is very similar to the one we used above for the network but instead applied directly to the group-by-individual matrix (incidence matrix). Our test statistic is going to be the mean edge weight variance (coefficient of variation in edge weights). We will constrain swaps to only be between groups recorded at the same time step.

Again we will use a Markov chain approach to generate our reference distribution.

```{r gbi_swaps, message=FALSE}

##Conduct edge rewiring while constraining swaps to be within social groups

#Write function to calculate coefficient of variation
cov_calc<-function(a){
  #a[a==0]<-NA
  return(sd(a,na.rm=T)/mean(a,na.rm=T))
}

#Calculate the coefficient of variation in the observed network
diag(group_mat)<-NA
obs_cov<-mean(apply(group_mat,1,cov_calc))

#We will have a burn-in of 500 swaps and then sample the network every 10 swaps until we have the full reference distribution. Here we calculate just a small version of one chain to save time, but this can be longer in reality.

#Set the parameters for the chain
burnin<-500
thin<-10
chain<-10000

#Set up the vector to store the reference distribution
perm_cov<-numeric()

#Reference GBI for conducting swaps
gbi_t<-groupings[[1]]
#We now conduct our swaps in a slower but easier to explain way so I can explain the logic as we go
for(i in 1:burnin){
  #sample an individual/grouping-event
  pind<-which(gbi_t>0,arr.ind=TRUE)
  tind1<-pind[sample(1:nrow(pind),1),]
  #record the day on which that individual/grouping-event occurred
  td<-which(day==day[tind1[1]])
  #sample a second individual/grouping-event that occurs on the same day
  pind2<-pind[which(pind[,1]%in%td),]
  tind2<-pind2[sample(1:nrow(pind2),1),]
  #If additional constraints are met then conduct swap
  if(tind1[1]!=tind2[1]&tind1[2]!=tind2[2]){
    if(gbi_t[tind1[1],tind2[2]]==0&gbi_t[tind2[1],tind1[2]]==0){
        gbi_t2<-gbi_t
        gbi_t2[tind2[1],tind1[2]]<-gbi_t[tind1[1],tind1[2]]
        gbi_t2[tind1[1],tind1[2]]<-gbi_t[tind2[1],tind1[2]]
        gbi_t2[tind1[1],tind2[2]]<-gbi_t[tind2[1],tind2[2]]
        gbi_t2[tind2[1],tind2[2]]<-gbi_t[tind1[1],tind2[2]]
        gbi_t<-gbi_t2
    }
  }
}

#We now continue our sampling but every 10 iterations we calculate the efficiency of the network. The counter helps add to the reference distribution.
counter<-1
for(i in 1:chain){
  #sample an individual/grouping-event
  pind<-which(gbi_t>0,arr.ind=TRUE)
  tind1<-pind[sample(1:nrow(pind),1),]
  #record the day on which that individual/grouping-event occurred
  td<-which(day==day[tind1[1]])
  #sample a second individual/grouping-event that occurs on the same day
  pind2<-pind[which(pind[,1]%in%td),]
  tind2<-pind2[sample(1:nrow(pind2),1),]
  #If additional constraints are met then conduct swap
  if(tind1[1]!=tind2[1]&tind1[2]!=tind2[2]){
    if(gbi_t[tind1[1],tind2[2]]==0&gbi_t[tind2[1],tind1[2]]==0){
        gbi_t2<-gbi_t
        gbi_t2[tind2[1],tind1[2]]<-gbi_t[tind1[1],tind1[2]]
        gbi_t2[tind1[1],tind1[2]]<-gbi_t[tind2[1],tind1[2]]
        gbi_t2[tind1[1],tind2[2]]<-gbi_t[tind2[1],tind2[2]]
        gbi_t2[tind2[1],tind2[2]]<-gbi_t[tind1[1],tind2[2]]
        gbi_t<-gbi_t2
    }
  }
  
  #Here we generate the social network and calculate the edge weight variance
  if(i%%thin==0){
    gmt<-asnipe::get_network(gbi_t)
    diag(gmt)<-NA
    perm_cov[counter]<-mean(apply(gmt,1,cov_calc))
    counter<-counter+1
  }
  
}

#Let's compare the observed coefficient of variation to the quantiles of the reference distribution
print(list(obs_cov,quantile(perm_cov,c(0.025,0.25,0.5,0.75,0.975))))

```

We can see that the edge weight variance in our social network is very different to what would be expected if the grouping events/interactions occurred among random individuals. Obviously this isn't the most interesting research question, but we can add constraints to the generation of our reference distribution to find processes that might be important.

You will also note an important step here is that when we can't make a swap we retain the current GBI until the next step in the Markov Chain. This is important in sampling from the reference distribution in an unbiased way. It is explained more in the link immediately below.

If you want to learn more about using permutations, and reference models more generally then [this](https://doi.org/10.1111/brv.12775) is a good place to start. Some parts (e.g. not highlighting the importance of multiple chains) are a little outdated now (animal social network analysis is changing fast!) but it still provides a nice overview and points out key pitfalls to avoid. There is also a tutorial than goes into more detail than we have time to here

However, now we've talked about the potential of reference models and permutations, we are going to change tack as for many of the questions we ask in animal social network analysis it is more effective to simply use the types of linear regression approaches we are more used to, or extensions of them designed for network analysis. An introduction to these methods comes below.

***
***

In subsequent sections we will focus on applying regression approaches to social networks, thinking about some of the issues we need to think about when doing so. Except when we are using specific R packages tailored to social network modelling I am going to use the R package [brms](https://paul-buerkner.github.io/brms/) for the example analyses here. The reasons for this is that brms provides a fantastic combination of being incredibly flexible and also user friendly. The other reason is that by using Bayesian approaches [handle issues with non-independence more robustly than frequentist approaches](https://www.nature.com/articles/s41598-017-18104-4) would in situations where it is difficult to control for non-independence introduce by working with networks.

***

## SESSION 3: PART 3

First we are going to start with nodal regressions that involve non-network traits as these are the simplest category of regressions, and largely will be similar to ones that you are used to. The main consideration with these types of nodal regressions is if there is non-independence related to individuals closely connected in the social network being more (or less!) similar than you would expect by chance. If you are confident this is not the case, then you can work with these types of regression analyses in the same you way for any other individual trait. However, if you want to account for the potential dependence between connected individuals, there are a couple of options:

1. You can adapt methods from spatial analyses or quantitative genetics (the [animal model](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2009.01639.x)) and directly use the network as a weighting matrix.

2. You can work with [network autocorrelation models](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12770). This allows considerable flexibility in how you model network-related covariance in traits, including the ability to model specific functional relationships and over different neighbourhoods (e.g. first-order vs. second-order connections). But it comes at the cost of subjectivity and decision-making!

We will demonstrate how each approach might work below using the `full_adj` network and `ind_data` data frame. We will test the hypothesis that the weight of individuals is associated with their sex and network centrality.

```{r nr1}

#We start by calculating a network measure and adding it to the dataframe
ind_data$degree<-igraph::degree(network)

##Without covariance issues our regression is simply
model1<-brm(formula=weight~sex+degree,data=ind_data)

#Check model summary
summary(model1)

```

Note that there is no effect of sex or degree centrality in this case - this is unsurprising as I simulated the data independently of each other. Feel free to test your own hypothesis in this dataset or simulate a new variable that is associated with one or more measures of individual network position.

I am also deliberately not focussing too much on either statistical inference or Bayesian model specification. These things are clearly important but extend beyond the remits of "social network analysis". So to save time and focus on the social network methods I am leaving them out for now. When working with networks I typically like to use full model inference or work with a subset of models with clear (potential) causal pathways - network analyses are already complex and it is easy to over-complicate them! There are lots of resources available online related to Bayesian modelling if you are unsure on this topic. You could also adjust many of these models for Frequentist inference too.

***

Now we will introduce the use of covariance matrices to this same regression.

```{r nr2}

##The next option is to use the social network as a covariance matrix
#We first need to convert the social network to be usable as a covariance matrix - it needs to be positive definite to allow this.
weight_matrix<-full_adj
diag(weight_matrix)<-1
#Here we use the nearest positive definite matrix approximated by the function nearPD from the Matrix package
weight_matrix<-as.matrix(Matrix::nearPD(weight_matrix,keepDiag=TRUE)$mat)
#Row and column names need to match our dataset
colnames(weight_matrix)<-rownames(weight_matrix)<-ind_data$id

#We can use a matrix regression approach to test that the approximated matrix is correlated with the social network - more on this statistical method later...
summary(sna::netlm(weight_matrix,full_adj))

#We can then fit our regression using gr() in brms and providing the matrix we calculated as the covariance matrix
model2<-brm(formula=weight~sex+degree+(1|gr(id,cov=weight_matrix)),data=ind_data,data2=list(weight_matrix=weight_matrix),chains=4,cores=4,iter=5000,warmup=1000)

#check model summary
summary(model2)

#Calculate the importance of network covariance
hyp <- "sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2) = 0"
hyp <- hypothesis(model2, hyp, class = NULL)
plot(hyp)

```

As above, we find no covariance here (the posterior distribution of `hyp` is very close to zero) because the dataset wasn't simulated this way in the first place. We can also see that the estimated effects of sex and degree are very similar to the first model.

***

The third option is the network autocorrelation modeling framework. I provide a couple of examples here, showing different ways we may introduce covariance structures. With NAMs we are specifying the dependencies between socially connected nodes ourselves.

```{r nr3}

#The first step of our NAM analysis is to calculate two new variables: 1) the mean weight of all immediately adjacency nodes (first-order neighbours); and 2) half the mean weight of all second-order neighbours.

#Work out first-order neighbours
egos1<-igraph::ego(network)

#Work-out second-order neighbours
egos2<-igraph::ego(network,order=2,mindist=2)

#Let's now calculate the variables
ind_data$fo_eff<-ind_data$so_eff<-rep(NA,nrow(ind_data))
#We'll use a loop here as it is easier to follow what is going on
#Note that because the order of individuals in our dataframe matches that in the matrix we can use some coding shortcuts here
#but be careful to make sure these match if you use this type of approach in your own analysis
for(i in 1:nrow(ind_data)){
  ind_data$fo_eff[i]<-mean(ind_data$weight[egos1[[i]]],na.rm=TRUE)
  ind_data$so_eff[i]<-0.5*mean(ind_data$weight[egos2[[i]]],na.rm=TRUE)
}

#First a model with just the effect of first-order neighbours
model3<-brm(formula=weight~sex+degree+fo_eff,data=ind_data)
#Check model summary
summary(model3)

#Now we can add the effect of second-order neighbours
model4<-brm(formula=weight~sex+degree+fo_eff+so_eff,data=ind_data)
#Check model summary
summary(model4)

```

This gives a basic insight into how network autocorrelations models work. The positive effect of the mean weight of first-order neighbours is a surprise - it wasn't simulated into the dataset, and may warrant further investigation. However, it is at odds with our `model2` results. This suggests that if we incorporated edge weights into our calculation of the first-order effect it may disappear. This may be something to try if you are feeling happy with the general idea of how these models work. Note also that we chose that our neighbourhood effects should be weighted by dividing through by path length - this is a common functional form to use in NAMs but you could equally pick something else. In fact, the terms you could include in this general framework are only really limited by your imagination - this subjectivity can be both a strength and a major weakness. Again, a key piece of advice is to keep your models simple and to only include autocorrelation terms that you have clear expectations to have an effect. It can also be useful for distinguish between exploratory analyses where you are looking to describe potentially interesting patterns in the data versus hypothesis testing where you are examining the ability of a pre-conceived model to explain variation in your study system.

Hopefully, these examples give a good grounding in approaches to incorporating social networks in regression-type analyses for non-network traits.

***
***

## SESSION 3: PART 4

That means the next step is to look at how we conduct nodal regressions when network traits (such as degree or other centrality measures) are the response variable. Note that as people move away from using permutation approaches for these types of analyses that this is an area that is in flux, and so the value of the ideas I teach here could change quickly!

One thing that I won't cover here, but is important to think about is also including the observation process in your model and propagating any uncertainty through to any subsequent regression analyses (note that this could also apply to Part 3 above). Currently, the best guidance to taking this approach for animal social networks is available [here](https://www.biorxiv.org/content/10.1101/2021.12.20.473541v2.full). Taking this type of approach makes it easier to control for variation in sampling effort between individuals or regions of the network, for example. It could also be possible to control for these directly when modelling social network measure by including them as a covariate - although note here that for some measures in particular you might expect these relationships to be non-linear favouring using GAMs or specifying the form of the relationship (see [here]() for more detail about this approach).

Currently, the most sensible approach to nodal regressions is to use Bayesian inference and not worry about using permutation methods. For example, a regression for a centrality measure could look like this:

```{r nr4}

ind_data$strength<-igraph::strength(network)
modelS1<-brm(formula=strength~sex,data=ind_data)
summary(modelS1)

```

One slightly painful aspect of social network measures for nodel regressions is that many of them require non-Gaussian error distributions to model well. To illustrate, our diagnostic plots for our model for strength are just fine, but if we use the same model for betweenness centrality things go wrong...

```{r nr5}

#Posterior predictive check for strength
pp_check(modelS1,ndraws=100)

#Add betweenness as a variable
ind_data$betweenness<-igraph::betweenness(network,weights=1/E(network)$weight)

#Fit same model but for betweenness
modelB1<-brm(formula=betweenness~sex,data=ind_data)

#Posterior predictive check for betweenness
pp_check(modelB1,ndraws=100)

```
Not only does our posterior not match the observed density, but it also predicts betweenness values below zero which isn't possible. In the case of betweenness centrality, this issue can be fixed using both a negative binomial family model and a zero-inflation component.

```{r nr6}

#Fit zero-inflated negative binomial model
modelB2<-brm(bf(betweenness~sex, zi~sex),data=ind_data,family=zero_inflated_negbinomial())

#Posterior predictive check for new model
pp_check(modelB2,ndraws=100)

```

The best model to use will depend on the network measure you are using and also aspects of the social system you are studying. For example, eigenvector centrality and clustering coefficient scale between 0 and 1, while(uncorrected) degree is a count that can only be discrete numbers. In general, you should be able to make a good start with knowledge of the measures and their distributions in your study system and refine from there.

You may also want to control for network dependency using random effect structures, e.g. using social groupings or by using individual-level random effects for repeated or longitudinal data. For example, for our dataset we could use group as a random effect.

```{r nr7}

#Fit zero-inflated negative binomial model
modelB3<-brm(bf(betweenness~sex+(1|group), zi~sex+(1|group)),data=ind_data,family=zero_inflated_negbinomial())
#Look at model summary
summary(modelB3)

```

However, there is an important caveat here. It might be tempting to use features of the network - such as social grouping you detect using community detection algorithms. But there is a technical issue here, which is that your network measure and the social communities both depend on the same set of social observations meaning that they weren't measured independetly of each other. This violates assumptions of the model fitting process meaning that approaches like this are best avoided.

In terms, of coping with more local sources of non-independence, you could consider using some of the approaches described for non-network traits such as covariance matrices - although this should be done with great care and only when you have specific hypotheses. Many social network structure will inherently contain covariance between connected individuals so it would be easy to misinterpret what these affects meant.

It is also possible to use permutations to control for non-independence and also for some sampling constraints, but this comes with a double caveat. The easiest (and most commonly used in the past) permutations to use are [no more effective in controlling for non-independence as the approaches described here](https://link.springer.com/article/10.1007/s00265-022-03254-x) (although completely harmless to use if used well) and more complex approaches such as datastream permutations are very hard to set up in a way that generates a meaningful null hypothesis to compare to (see [here](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13508)). You could design permutation tests that are effective, but by that stage it is probably more worth investing time in a very well-designed statistical model.

Just for fun here is the strength model from above but with the covariance matrix added.

```{r nr8}

#Fit model
modelS2<-brm(formula=strength~sex+(1|gr(id,cov=weight_matrix)),data=ind_data,data2=list(weight_matrix=weight_matrix),chains=4,cores=4,iter=5000,warmup=1000)
#check model summary
summary(modelS2)

#Calculate the importance of network covariance
hyp <- "sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2) = 0"
hyp <- hypothesis(modelS2, hyp, class = NULL)
plot(hyp)

```

Here we have a nice example of the point I was making, we detect a strong signal for covariance in strength even though this was not specifically included in the simulations. This has probably arisen because in our underlying network highly-connected individuals are more likely to be connected to other highly-connected individuals. Please feel free to try fitting this model for other network measures, or to think about how their values may covary within the network.

Finally, some of you may be interested in looking at spatial or temporal variation in social network measures. In this case, I'd very much recommend looking at some of Greg Albery's recent work who has used [INLA](https://www.r-inla.org/) to [model spatial and temporal variation in individual social centrality](https://onlinelibrary.wiley.com/doi/full/10.1111/ele.13684).

***
***

## SESSION 3: PART 5

Now we have covered some examples of nodal regressions, it is time to think instead about analyses focused directly on the edges of the network or the network object. These can also be invaluable for testing hypotheses related to the structure of animal social networks.

***

The first useful dyadic regression to know are matrix regression approaches adapted for social networks. These can be run using `netlm` in the R package [sna](https://cran.r-project.org/web/packages/sna/sna.pdf). These approaches are effective for testing the association between social networks for different types of behaviours or for testing the association between social networks and other types of matrix such as home range overlap or distance matrices for spatial analysis, or a relatedness matrix to test how well genetic relationships explain social relationships.

We first will use this approach to compare the `full_adj` network with the network of grouping events generated using it (`group_mat`). We therefore predict a strong positive relationship between these two matrices. `netlm` uses a permutation approach, and you can specify the one you use using the `nullhyp` argument. In general, I would recommed using `qapspp` as a starting point, but it is helpful to familiarise yourself with the other options.

```{r mrqap1}

#Fit model - we keep reps lower here but you may want to increase it for a formal analysis
net_model1<-sna::netlm(y=group_mat,x=full_adj,nullhyp="qapspp",reps=1000)

#Examine model summary
summary(net_model1)

```

The relationship between the two network is as we would expect. The useful thing about `netlm` is we can use it to have multiple networks or matrices as explanatory variables to test what explains social network structure best. Below we simulate two matrices for home range overlap and relatedness to demonstrate how to implement this.

```{r mrqap2}

#Simulate our matrices of home range overlap and relatedness
hr_overlap<-relatedness<-matrix(0,nr=100,nc=100)
for (i in 1:(ncol(relatedness)-1)){
  for(j in (i+1):ncol(relatedness)){
    relatedness[i,j]<-relatedness[j,i]<-sample(c(0.5,0.25,runif(1,min=-0.1,max=0.25)),1,prob=c(0.02,0.08,0.9))
    hr_overlap[i,j]<-hr_overlap[j,i]<-ifelse(ind_data$group[i]==ind_data$group[j],runif(1,min=0.6,max=0.9),runif(1,min=0.1,max=0.65))
  }
}

#Fit model - we keep reps lower here but you may want to increase it for a formal analysis
net_model2<-sna::netlm(y=group_mat,x=list(hr_overlap,relatedness),nullhyp="qapspp",reps=1000)

#Examine model summary
summary(net_model2)

```

With this second analysis we explain ~25% of variation in our data, with home range overlap having a statistically clear association with social network structure (the weight of edges). Take a look at how the data were simulated and see if you can work out why home range overlap is associated with social structure but home range overlap isn't.

***

`netlm` can only get us so far with dyadic regressions. A more sophisticated approach that allows us to incorporate a greater range of explanatory variables uses what are known as multiple membership random effects. Multiple membership random effects allow us to estimate the variation attributable to particular individuals or nodes when we only have data on dyadic relationships (each of which consists of two individuals or nodes). Fortunately, **brms** (and various other R packages) make fitting these models relatively straightforward. We demonstrate an example below using our `full_adj` network.

We will test two hypotheses - that edge strength is influenced by a) social group membership; and b) whether the dyads are female-female, female-male or male-male. Note that our second hypothesis here is very similar to the one we first tested using permutation approaches (and could be adapted to be the same) - often there are multiple ways to test the same hypothesis with social network analysis (as with other analyses).


```{r multiplemembership}

#First we need to make an edge list which is equivalent to long-form data for dyadic regressions
#Make empty edgelist
el_network<-matrix(NA,nr=0.5*(nrow(full_adj)^2-nrow(full_adj)),nc=3)
#Fill edgelist (there are some shortcuts used here because our matrix rows/columns have the same names as our individuals - be careful when using this code)
count<-1
for(i in 1:(ncol(full_adj)-1)){
  for(j in (i+1):ncol(full_adj)){
    el_network[count,1]<-i
    el_network[count,2]<-j
    el_network[count,3]<-full_adj[i,j]
    count<-count+1
  }
}
el_network<-as.data.frame(el_network)
names(el_network)<-c("id1","id2","weight")

#Then we need to calculate our new explanatory variables - we do this as a loop to help display the logic
el_network$same_group<-el_network$dyad_sex<-rep(NA,nrow(el_network))
for(i in 1:nrow(el_network)){
  ifelse(ind_data$group[ind_data$id==el_network$id1[i]]==ind_data$group[ind_data$id==el_network$id2[i]],
  el_network$same_group[i]<-1,
  el_network$same_group[i]<-0)
  el_network$dyad_sex[i]<-paste0(ind_data$sex[ind_data$id==el_network$id1[i]],ind_data$sex[ind_data$id==el_network$id2[i]])
}
#Swap MF to also be FM
el_network$dyad_sex[el_network$dyad_sex=="MF"]<-"FM"

#Make dyad_sex a factor
el_network$dyad_sex<-as.factor(el_network$dyad_sex)

#We can then fit the model
mm_model1<-brm(bf(weight~same_group+dyad_sex+(1|mm(id1,id2)),zi~same_group+dyad_sex+(1|mm(id1,id2))),data=el_network,family=zero_inflated_beta(),chains=4,cores=4)

#Look at the summary
summary(mm_model1)

#And look at the posterior predictive check
pp_check(mm_model1,ndraws=100)

```

The model finds that dyad_sex is not an important variable in explaining our network structure. It does find a really big effect of whether two individuals were in the same group or not, but only in the zero-inflation part of the model. Reflect on how we simulated network and see whether you can work out why this is.

***

A natural extension to multiple membership random effects for directed networks with repeated interactions is what is known as the [social relations model](https://doi.org/10.1016/S0065-2601(08)60144-6). This incorporates individual-level random effects related to sociality (out-going edges) and popularity (in-going edges). Here we are going to generate a repeated interactions version of our directed network object and then demonstrate how to fit a social relations model.

It is a set of 6 directed networks simulated using the same algorithm we used for `dir_net` and `dir_mat` in Session 2. The objects we need from the simulations are `dir_mats.RDS` and `ind_data2.RDS`.

```{r sr_gen, class.source = "fold-hide",eval=FALSE}

#Create 40 individuals and give them each a size
inds2<-seq(1,40,1)
size2<-rnorm(40,0,1)
sex2<-sample(c("M","F"),40,replace=TRUE)

#Create a list to store networks
dir_mats<-list()

#Create six directed edgelists for their interactions

for(reps in 1:6){

el_dom<-matrix(NA,nr=2000,nc=2)
for(i in 1:nrow(el_dom)){
  t_i1<-sample(inds2,1,prob=0.1+size2+abs(min(size2)))
  t_i2<-sample(inds2[-t_i1],1)
  size_diff<-size2[t_i1]-size2[t_i2]
  winner<-rbinom(1,1,boot::inv.logit(size_diff))
  if(winner==1){
    el_dom[i,1]<-t_i1
    el_dom[i,2]<-t_i2
  }
  if(winner==0){
    el_dom[i,2]<-t_i1
    el_dom[i,1]<-t_i2
  }
}

dir_net<-graph_from_edgelist(el_dom,directed=TRUE)
E(dir_net)$weight<-1
dir_net<-simplify(dir_net, edge.attr.comb=list(weight="sum"))
dir_mat<-as_adjacency_matrix(dir_net,attr="weight",sparse=FALSE)

dir_mats[[reps]]<-dir_mat

}

ind_data2<-data.frame(inds2,size2,sex2)
saveRDS(ind_data2,"ind_data2.RDS")
saveRDS(dir_mats,"dir_mats.RDS")

```

With this dataset we can now fit a social relations model. 

```{r socialrelations, cache=TRUE}

#Read in data
ind_data2<-readRDS("ind_data2.RDS")
dir_mats<-readRDS("dir_mats.RDS")

#First we need to create our dataset in long form. The code below takes the same approach as before but applies it to all six matrices.
#Make empty edgelist
el_dir<-matrix(NA,nr=6*(nrow(ind_data2)^2),nc=3)
#Fill edgelist (there are some shortcuts used here because our matrix rows/columns have the same names as our individuals - be careful when using this code)
count<-1
for(reps in 1:6){
  for(i in 1:ncol(dir_mats[[reps]])){
    for(j in 1:ncol(dir_mats[[reps]])){
      el_dir[count,1]<-i
      el_dir[count,2]<-j
      el_dir[count,3]<-dir_mats[[reps]][i,j]
      count<-count+1
    }
  }
}

#Finalise dataframe
el_dir<-as.data.frame(el_dir)
names(el_dir)<-c("id1","id2","weight")
el_dir$network<-rep(1:6,each=length(dir_mats[[1]]))

#Remove diagonals (which are not relevant)
el_dir<-el_dir[el_dir$id1!=el_dir$id2,]

#Create a column with a dyad identifier
el_dir$dyad<-paste0(el_dir$id1,"-",el_dir$id2)

#Fit the model
sr_model1<-brm(bf(weight~(1|id1)+(1|id2)+(1|dyad)),data=el_dir,family=poisson(),chains=4,cores=4,iter=4000,warmup=1000)

#Model summary
summary(sr_model1)

#posterior predictive check
pp_check(sr_model1,ndraws=100)

```

Some potential challenges if you are feeling confident... See if you can work out why the posterior predictive check looks like this. What do you think the explanation is for the relative importance of the different random effects in the model?

Note that we could instead make a version where the dyad or relationship effect is undirected - give this a go if you want! We could also add nodal or dyadic fixed effects to this model if we had hypotheses to test - in this set-up the nodal effects can apply specifically to a sender or receiver of an edge. Again this is something you could try using the `ind_data2 ` dataframe associated with these networks. For an undirected network you could use multiple-membership random effect as introduced above rather than separate sender and receiver effects.

This was a very quick tour of different regression-based approaches for social network analysis. In the final Session (or the end of this Session) I will introduce just a couple of specialist network models that might be most useful to you when working with animal network data.

***
***
***